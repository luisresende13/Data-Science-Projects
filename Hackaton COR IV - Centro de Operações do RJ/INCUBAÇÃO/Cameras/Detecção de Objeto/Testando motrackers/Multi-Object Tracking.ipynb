{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72fccdd0-6f0e-4fdc-8504-74ddc92cf058",
   "metadata": {},
   "source": [
    "# Multiple object detection and tracking with `MOTracker` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575ea3d-0d51-423c-9c42-c0f31047321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install motrackers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f1652-5b8b-4ea0-b18c-7c7b370dc627",
   "metadata": {},
   "source": [
    "MOTracker GitHub Repository: https://github.com/adipandas/multi-object-tracker/blob/master/examples/example_notebooks/mot_YOLOv3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f677dec3-739d-4b00-a91b-661038f37fad",
   "metadata": {},
   "source": [
    "#### Download MobileNet SSD .prototxt and .caffemodel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a259b9b3-414e-4d13-99a3-5e1c10c2dcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MobileNetSSD_deploy.prototxt', <http.client.HTTPMessage at 0x19c98189d80>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = \"https://drive.google.com/u/0/uc?id=0B3gersZ2cHIxRm5PMWRoTkdHdHc&export=download\"\n",
    "filename = \"../Modelos/MobileNetSSD/MobileNetSSD_deploy.caffemodel\"\n",
    "\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/chuanqi305/MobileNet-SSD/daef68a6c2f5fbb8c88404266aa28180646d17e0/MobileNetSSD_deploy.prototxt\"\n",
    "filename = \"../Modelos/MobileNetSSD/MobileNetSSD_deploy.prototxt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ecb658-259e-4958-9535-4b68941c095f",
   "metadata": {},
   "source": [
    "### Write video from url stream with yolo version 3 + `motracker` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6752b31d-2616-4645-9422-c992d72d56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import urllib.request\n",
    "import json\n",
    "from motrackers import CentroidTracker\n",
    "\n",
    "# Download the YOLOv3 model files\n",
    "yolov3_weights_filename = \"../Modelos/yolov3/yolov3.weights\"\n",
    "yolov3_config_filename = \"../Modelos/yolov3/yolov3.cfg\"\n",
    "\n",
    "# Download the JSON file for class names\n",
    "class_names_filename = \"../Modelos/yolov3/coco.names\"\n",
    "\n",
    "# Load the YOLOv3 model\n",
    "net = cv2.dnn.readNetFromDarknet(yolov3_config_filename, yolov3_weights_filename)\n",
    "\n",
    "# Load the class names\n",
    "with open(class_names_filename, \"r\") as f:\n",
    "    class_names = f.read().splitlines()\n",
    "\n",
    "# URL of the video stream\n",
    "video_url = \"http://187.111.99.18:9004/?CODE=1646\"\n",
    "\n",
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "# Define the output video writer\n",
    "output_filename = \"output.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "fps = 3\n",
    "width = 854\n",
    "height = 480\n",
    "output = cv2.VideoWriter(output_filename, fourcc, fps, (width, height))\n",
    "\n",
    "# Initialize the tracker (CentroidTracker in this case)\n",
    "tracker = CentroidTracker()\n",
    "\n",
    "# seconds to capature\n",
    "seconds = 10\n",
    "\n",
    "# number of frames captured\n",
    "i = 0\n",
    "\n",
    "# Process each frame of the video\n",
    "while i <= fps * seconds:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    # Perform object detection with YOLOv3\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    \n",
    "    # Extract bounding boxes, confidences, and class IDs from the detections\n",
    "    detection_bboxes = []\n",
    "    detection_confidences = []\n",
    "    detection_class_ids = []\n",
    "    \n",
    "    for detection in detections:\n",
    "        scores = detection[5:]\n",
    "        class_id = scores.argmax()\n",
    "        confidence = scores[class_id]\n",
    "        \n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            bbox_width = int(detection[2] * width)\n",
    "            bbox_height = int(detection[3] * height)\n",
    "            \n",
    "            # Convert center coordinates to top-left coordinates\n",
    "            bbox_left = int(center_x - bbox_width / 2)\n",
    "            bbox_top = int(center_y - bbox_height / 2)\n",
    "            \n",
    "            detection_bboxes.append([bbox_left, bbox_top, bbox_width, bbox_height])\n",
    "            detection_confidences.append(confidence)\n",
    "            detection_class_ids.append(class_id)\n",
    "    \n",
    "    # Update the tracker with the detection results\n",
    "    output_tracks = tracker.update(detection_bboxes, detection_confidences, detection_class_ids)\n",
    "    \n",
    "    # Draw bounding boxes and annotations on the frame\n",
    "    for track in output_tracks:\n",
    "        frame_id, track_id, bbox_left, bbox_top, bbox_width, bbox_height, confidence, _, _, _ = track\n",
    "        class_name = class_names[track_id]\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (int(bbox_left), int(bbox_top)), (int(bbox_left + bbox_width), int(bbox_top + bbox_height)), (0, 255, 0), 2)\n",
    "        \n",
    "        # Annotate with class name and track ID\n",
    "        label = f\"{class_name} {track_id}\"\n",
    "        cv2.putText(frame, label, (int(bbox_left), int(bbox_top - 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    # Write the frame with annotations to the output video file\n",
    "    output.write(frame)\n",
    "    \n",
    "    # Display the frame\n",
    "    # cv2.imshow(\"Video\", frame)\n",
    "    # if cv2.waitKey(1) == ord(\"q\"):\n",
    "        # break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "output.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b74355-dbbf-4bf6-9145-e1c347fa401a",
   "metadata": {},
   "source": [
    "### Write video from url stream with: MobileNet SSD from .prototxt and .caffemodel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9050c69-ca40-4a66-9872-7b36bd2901c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "def save_video(url, model_filename, prototxt_filename, json_filename, seconds):\n",
    "\n",
    "    # Load the model\n",
    "    net = cv2.dnn.readNetFromCaffe(prototxt_filename, model_filename)\n",
    "\n",
    "    # Load the class names from the JSON file\n",
    "    with open(json_filename, 'r') as f:\n",
    "        class_names = json.load(f)\n",
    "\n",
    "    # Create a VideoCapture object\n",
    "    cap = cv2.VideoCapture(url)\n",
    "\n",
    "    # Define the output video writer\n",
    "    output_filename = \"output.avi\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    fps = 3  # Set the desired FPS\n",
    "    width = 854  # Set the width of the frame (854 pixels)\n",
    "    height = 480  # Set the height of the frame (480 pixels)\n",
    "    output = cv2.VideoWriter(output_filename, fourcc, fps, (width, height))\n",
    "\n",
    "    # Calculate the number of frames to capture based on the desired seconds\n",
    "    frame_count = int(fps * seconds)\n",
    "\n",
    "    # Process each frame of the video\n",
    "    for i in range(frame_count):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (300, 300))\n",
    "        # Perform object detection on the frame\n",
    "        blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (127.5, 127.5, 127.5), swapRB=True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "\n",
    "        # Draw bounding boxes around detected objects and annotate with class names\n",
    "        for j in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, j, 2]\n",
    "            if confidence > 0.5:\n",
    "                class_id = int(detections[0, 0, j, 1])\n",
    "                class_name = class_names[class_id]\n",
    "                x = int(detections[0, 0, j, 3] * width)\n",
    "                y = int(detections[0, 0, j, 4] * height)\n",
    "                w = int(detections[0, 0, j, 5] * width)\n",
    "                h = int(detections[0, 0, j, 6] * height)\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, class_name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame with annotations to the output video file\n",
    "        output.write(frame)\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    output.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# URL of the video stream\n",
    "video_url = \"http://187.111.99.18:9004/?CODE=1646\"\n",
    "\n",
    "# Set the desired seconds for video processing\n",
    "seconds = 10\n",
    "\n",
    "# Set the paths for model, prototxt, and JSON files\n",
    "model_filename = \"../Modelos/MobileNetSSD/MobileNetSSD_deploy.caffemodel\"\n",
    "prototxt_filename = \"../Modelos/MobileNetSSD/MobileNetSSD_deploy.prototxt\"\n",
    "json_filename = \"../Modelos/MobileNetSSD/ssd_mobilenet_caffe_names.json\"\n",
    "\n",
    "# Call the save_video function\n",
    "save_video(video_url, model_filename, prototxt_filename, json_filename, seconds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4427ab-0ea2-4708-879a-341d6618b92d",
   "metadata": {},
   "source": [
    "### Write video of Yolo v3 from URL cv2.VideoCapture video stream · Using CUDA backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24825b9e-0e97-48f4-a3d6-4214c39de138",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\layers\\convolution_layer.cpp:392: error: (-215:Assertion failed) !blobs.empty() || inputs.size() > 1 in function 'cv::dnn::ConvolutionLayerImpl::getMemoryShapes'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mgetLayerNames()\n\u001b[0;32m     73\u001b[0m output_layers \u001b[38;5;241m=\u001b[39m [layer_names[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mgetUnconnectedOutLayers()]\n\u001b[1;32m---> 74\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Process the output to get the object detections and bounding boxes\u001b[39;00m\n\u001b[0;32m     77\u001b[0m class_ids \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.7.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\layers\\convolution_layer.cpp:392: error: (-215:Assertion failed) !blobs.empty() || inputs.size() > 1 in function 'cv::dnn::ConvolutionLayerImpl::getMemoryShapes'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv3 weights and configuration\n",
    "# model_filename = \"../Modelos/MobileNetSSD/MobileNetSSD_deploy.caffemodel\"\n",
    "# prototxt_filename = \"../Modelos/MobileNetSSD/MobileNetSSD_deploy.prototxt\"\n",
    "# net = cv2.dnn.readNetFromCaffe(prototxt_filename, model_filename)\n",
    "\n",
    "net = cv2.dnn.readNet(\"../Modelos/yolov3/yolov3.weights\", \"../Modelos/yolov3/yolov3.cfg\")\n",
    "\n",
    "# Enable GPU acceleration\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n",
    "\n",
    "# URL for video stream\n",
    "url = \"http://187.111.99.18:9004/?CODE=1646\"\n",
    "\n",
    "# Create a VideoCapture object\n",
    "cap = cv2.VideoCapture(url)\n",
    "\n",
    "# Check if the video capture is successful\n",
    "if not cap.isOpened():\n",
    "    print(\"Failed to open the video stream.\")\n",
    "    exit()\n",
    "\n",
    "# Function to draw bounding boxes and labels on the image\n",
    "def draw_predictions(image, class_ids, confidences, boxes):\n",
    "    for class_id, confidence, box in zip(class_ids, confidences, boxes):\n",
    "        x, y, w, h = box\n",
    "        label = class_names[class_id]\n",
    "        color = colors[class_id]\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), color, 2)\n",
    "\n",
    "        # Draw label and confidence\n",
    "        text = f\"{label}: {confidence:.2f}\"\n",
    "        cv2.putText(image, text, (x, y-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "# Load class names for the labels\n",
    "with open(\"../Modelos/yolov3/coco.names\", \"r\") as f:\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Generate random colors for each class\n",
    "colors = np.random.uniform(0, 255, size=(len(class_names), 3))\n",
    "\n",
    "# Parameters\n",
    "seconds = 10  # Number of seconds to capture\n",
    "fps = 3 # Number of frames per second\n",
    "\n",
    "# Define the codec and create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "writer = cv2.VideoWriter('output.avi', fourcc, fps, (512, 512))\n",
    "\n",
    "# Variable to keep track of elapsed frames\n",
    "frame_count = 0\n",
    "\n",
    "while frame_count < seconds * fps:  # 20 frames per second\n",
    "    # Read the next frame from the video stream\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if no frame is captured\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize the frame to a common size\n",
    "    frame = cv2.resize(frame, (512, 512))\n",
    "\n",
    "    # Perform object detection on the frame\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layer_names = net.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    outputs = net.forward(output_layers)\n",
    "\n",
    "    # Process the output to get the object detections and bounding boxes\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.9:\n",
    "                center_x = int(detection[0] * frame.shape[1])\n",
    "                center_y = int(detection[1] * frame.shape[0])\n",
    "                width = int(detection[2] * frame.shape[1])\n",
    "                height = int(detection[3] * frame.shape[0])\n",
    "                left = int(center_x - width / 2)\n",
    "                top = int(center_y - height / 2)\n",
    "                class_ids.append(class_id)\n",
    "                confidences.append(float(confidence))\n",
    "                boxes.append([left, top, width, height])\n",
    "\n",
    "    # Draw bounding boxes and labels on the frame\n",
    "    draw_predictions(frame, class_ids, confidences, boxes)\n",
    "\n",
    "    # Write the frame to the output video file\n",
    "    writer.write(frame)\n",
    "\n",
    "    # Display the frame\n",
    "    # cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Increment the frame count\n",
    "    frame_count += 1\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        # break\n",
    "\n",
    "# Release the video capture object, close the output video file, and destroy any open windows\n",
    "cap.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
