{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video processing pipeline to concat and accelerate flood videos from Google Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luisr\\Desktop\\Repositories\\Data Science Projects\\Hackaton COR IV - Centro de Operações do RJ\\INCUBAÇÃO\\Cameras\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import clear_output as co\n",
    "from time import time\n",
    "\n",
    "# Simples class to report execution time\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time()\n",
    "    def end(self, decimals=4):\n",
    "        end = time() - self.start\n",
    "        print('\\n* TIME TO EXECUTE:', round(end, decimals), 's')\n",
    "        \n",
    "# Get blob count, bytes and names from Google Cloud Storage bucket\n",
    "\n",
    "def gcs_list_folder(folder, ext, bucket_name, print_each=10):\n",
    "    prefix = folder\n",
    "    delimiter = None\n",
    "    names = []\n",
    "    timer = Timer()\n",
    "    blobs = gcs.list_blobs(prefix, delimiter, bucket_name)\n",
    "    for i, blob in enumerate(blobs):\n",
    "        if blob.name.endswith(ext):\n",
    "            names.append([blob.name, blob.size])\n",
    "        if print_each is not None and (i + 1) % print_each == 0:\n",
    "            print(f'\\n- Blobs Searched: {i + 1}'); co(True)\n",
    "    names = pd.DataFrame(names, columns=['blob_name', 'bytes']) # build blobs dataframe\n",
    "    print(f'\\n * FOLDER: {folder} · EXT: {ext}')\n",
    "    print(f'\\n- Blobs Searched: {i + 1}')\n",
    "    print(f'\\n  · Blobs (Matched): {len(names)}')\n",
    "    print(f'\\n  · Giga Bytes (Matched): {round(names[\"bytes\"].sum() / 1e9, 3)} GB')\n",
    "    timer.end() # prints time to execute\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline methods set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Google Cloud Storage wrapper module and define storage instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.googlecloudstorage import GCS\n",
    "\n",
    "sa_json = '../../../../Apps/APIs/octa-api/credentials/octacity-iduff.json'\n",
    "user_project = None\n",
    "default_bucket_name = 'flood-video-collection'\n",
    "\n",
    "gcs = GCS(sa_json, user_project, default_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write videos with open-cv set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.video import VideoAnnotator\n",
    "\n",
    "# Video writer class instance\n",
    "\n",
    "writer = VideoAnnotator(fps=3, shape=(854, 480), codec='mp4v')\n",
    "\n",
    "# Accelerated video writer class instance\n",
    "\n",
    "writer_speed = VideoAnnotator(fps=24, shape=(854, 480), codec='mp4v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video writer class funcitonality\n",
    "1. Add running clock to video files\n",
    "2. Concatenate video files from nested folders\n",
    "3. Accelerate video files from nested folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 · Pipeline parameters set up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download from Google Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " - prefix: Folder in any level of the bucket containing sub-folders with videos to feed the pipeline\n",
    " * Note: Forward trailing slashses, i.e. `/`, at the end of `prefix` limits results to folders\n",
    "matching exactly to `prefix`. Otherwise, matches any folder or blob that contains `prefix`.\n",
    "\"\"\"\n",
    "\n",
    "prefix = 'polygons/flood-probability/'\n",
    "delimiter = None\n",
    "\n",
    "bucket_name = 'flood-video-collection' # collection bucket name\n",
    "folder = 'Dados/flood-video-collection' # bucket collection destination folder\n",
    "\n",
    "overwrite_download = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate videos timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'Dados/flood-video-collection'  # local collection source folder\n",
    "to_folder = 'Dados/flood-video-collection-stamped'  # `time-stamped` local collection source folder\n",
    "ext = '.mp4' # video file format to search for in nested folders\n",
    "overwrite_annot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate and accelerate videos from folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'Dados/flood-video-collection-stamped' # `time-stamped` local collection source folder\n",
    "to_base_folder = 'Dados/flood-video-collection-date' # concatenated local collection destination folder\n",
    "overwrite_concat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General purpose parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = '.mp4'\n",
    "report_freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.1 · Count blobs and download bytes · ***Preparation Step***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * FOLDER: polygons/flood-probability/ · EXT: .mp4\n",
      "\n",
      "- Blobs Searched: 712\n",
      "\n",
      "  · Blobs (Matched): 712\n",
      "\n",
      "  · Giga Bytes (Matched): 0.901 GB\n",
      "\n",
      "* TIME TO EXECUTE: 0.5814 s\n"
     ]
    }
   ],
   "source": [
    "blobs = gcs_list_folder(prefix, ext, bucket_name, print_each=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 · Download  blobs in Cloud Storage bucket to folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download blobs in `bucket_name` matching `prefix` to local `folder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREFIX: polygons/flood-probability/ · RUNNING: 9.9 min · RATE: 0.8342 s / file · FINISH-ESTIMATE: 0.0 min · PROGRESS: 710/712 · DOWNLOADS: 710/712\n",
      "\n",
      "* TIME TO EXECUTE: 593.9715 s\n"
     ]
    }
   ],
   "source": [
    "timer = Timer()\n",
    "\n",
    "gcs.download_to_folder(\n",
    "    folder, prefix, delimiter, bucket_name,\n",
    "    overwrite_download, report_freq\n",
    ")\n",
    "\n",
    "timer.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 · Annotate videos with dinamic timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add clock timestamp to nested video files in `folder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIDEO TIMESTAMP ANNOTATION · DONE: 15230/15236 · SUCCESS: 712/15236\n",
      "ANNOTATE VIDEO TIMESTAMP FAILED. FILE ALREADY EXISTS · FILE: Dados/flood-video-collection-stamped/rivers/manual/3323/3323/CODE3323 2023-04-07 19-55-10.mp4\n",
      "ANNOTATE VIDEO TIMESTAMP FAILED. FILE ALREADY EXISTS · FILE: Dados/flood-video-collection-stamped/rivers/manual/3323/3323/CODE3323 2023-04-07 20-00-06.mp4\n",
      "ANNOTATE VIDEO TIMESTAMP FAILED. FILE ALREADY EXISTS · FILE: Dados/flood-video-collection-stamped/rivers/manual/3323/3323/CODE3323 2023-04-07 20-05-03.mp4\n",
      "ANNOTATE VIDEO TIMESTAMP FAILED. FILE ALREADY EXISTS · FILE: Dados/flood-video-collection-stamped/rivers/manual/3323/3323/CODE3323 2023-04-07 20-14-27.mp4\n",
      "ANNOTATE VIDEO TIMESTAMP FAILED. FILE ALREADY EXISTS · FILE: Dados/flood-video-collection-stamped/rivers/manual/3323/3323/CODE3323 2023-04-07 20-29-58.mp4\n",
      "ANNOTATE VIDEO TIMESTAMP FAILED. FILE ALREADY EXISTS · FILE: Dados/flood-video-collection-stamped/rivers/manual/3323/3323/CODE3323 2023-04-07 20-53-02.mp4\n",
      "\n",
      "* TIME TO EXECUTE: 350.8879 s\n"
     ]
    }
   ],
   "source": [
    "timer = Timer()\n",
    "\n",
    "writer.write_timestamp_to_nested_videos(folder, to_folder, ext, overwrite_annot, report_freq)\n",
    "\n",
    "timer.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 · Concatenate and accelerate videos from folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate videos by date from nested folders in `base_folder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCAT VIDEOS BY DATE FROM NESTED FOLDERS · DONE: 430/433 · FOLDER: rivers/manual/1487\n",
      "CONCAT VIDEOS BY DATE FROM FOLDER (FAILED) · FILE ALREADY EXISTS · FILE: CODE1487 2023-04-07.mp4\n",
      "CONCAT VIDEOS BY DATE FROM FOLDER (FAILED) · FILE ALREADY EXISTS · FILE: CODE3323 2023-04-07.mp4\n",
      "CONCAT VIDEOS BY DATE FROM FOLDER (FAILED) · FILE ALREADY EXISTS · FILE: CODE3323 2023-04-08.mp4\n",
      "CONCAT VIDEOS BY DATE FROM FOLDER (FAILED) · FILE ALREADY EXISTS · FILE: CODE3323 2023-04-07.mp4\n",
      "\n",
      "* TIME TO EXECUTE: 293.8745 s\n"
     ]
    }
   ],
   "source": [
    "timer = Timer()\n",
    "\n",
    "writer_speed.concatenate_videos_by_date_from_nested_folders(\n",
    "    base_folder, to_base_folder, ext, overwrite_concat, report_freq\n",
    ")\n",
    "\n",
    "timer.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
