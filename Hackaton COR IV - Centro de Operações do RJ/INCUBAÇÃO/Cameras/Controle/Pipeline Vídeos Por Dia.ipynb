{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video processing pipeline to concat and accelerate flood videos from Google Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose base directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luisr\\Desktop\\Repositories\\Data Science Projects\\Hackaton COR IV - Centro de Operações do RJ\\INCUBAÇÃO\\Cameras\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# Simples class to report execution time\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self):\n",
    "        self.start = time()\n",
    "    def end(self, decimals=4):\n",
    "        end = time() - self.start\n",
    "        print('\\n* TIME TO EXECUTE:', round(end, decimals), 's')\n",
    "        \n",
    "# Get blob count, bytes and names from Google Cloud Storage bucket\n",
    "\n",
    "def gcs_list_folder(folder, ext, bucket_name, print_each=10):\n",
    "    prefix = folder\n",
    "    delimiter = None\n",
    "    names = []\n",
    "    timer = Timer()\n",
    "    blobs = gcs.list_blobs(prefix, delimiter, bucket_name)\n",
    "    for i, blob in enumerate(blobs):\n",
    "        if blob.name.endswith(ext):\n",
    "            names.append([blob.name, blob.size])\n",
    "        if print_each is not None and (i + 1) % print_each == 0:\n",
    "            print(f'\\n- Blobs Searched: {i + 1}'); co(True)\n",
    "    names = pd.DataFrame(names, columns=['blob_name', 'bytes']) # build blobs dataframe\n",
    "    if print_each is not None:\n",
    "        print(f'\\n- Blobs Searched: {i + 1}')\n",
    "        print(f'\\n  · Blobs (Matched): {len(names)}')\n",
    "        print(f'\\n  · Giga Bytes (Matched): {round(names[\"bytes\"].sum() / 1e9, 3)} GB')\n",
    "        timer.end() # prints time to execute\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline methods set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Google Cloud Storage wrapper module and define storage instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.googlecloudstorage import GCS\n",
    "\n",
    "sa_json = '../../../../Apps/APIs/octa-api/credentials/octacity-iduff.json'\n",
    "user_project = None\n",
    "default_bucket_name = 'flood-video-collection'\n",
    "\n",
    "gcs = GCS(sa_json, user_project, default_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write videos with opencv set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luisr\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from modules.video import VideoWriter\n",
    "\n",
    "# Video writer class instance\n",
    "\n",
    "writer = VideoWriter(fps=3, shape=(854, 480), codec='mp4v')\n",
    "\n",
    "# Accelerated video writer class instance\n",
    "\n",
    "writer_speed = VideoWriter(fps=24, shape=(854, 480), codec='mp4v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video writer class funcitonality\n",
    "1. Add running clock to video files\n",
    "2. Concatenate video files from nested folders\n",
    "3. Accelerate video files from nested folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Count blobs with .mp4 extension and total file bytes of download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import clear_output as co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pipeline Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 · Pipeline parameters set up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download from Google Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " - prefix: Folder in any level of the bucket containing sub-folders with videos to feed the pipeline\n",
    " * Note: Forward trailing slashses, i.e. `/`, at the end of `prefix` limits results to folders\n",
    "matching exactly to `prefix`. Otherwise, matches any folder or blob that contains `prefix`.\n",
    "\"\"\"\n",
    "\n",
    "prefix = 'comando/'\n",
    "delimiter = None\n",
    "\n",
    "bucket_name = 'flood-video-collection' # collection bucket name\n",
    "folder = 'Dados/flood-video-collection' # bucket collection destination folder\n",
    "\n",
    "overwrite_download = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate videos timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'Dados/flood-video-collection'  # local collection source folder\n",
    "to_folder = 'Dados/flood-video-collection-stamped' # `time-stamped` local collection source folder\n",
    "ext = '.mp4' # video file format to search for in nested folders\n",
    "overwrite_annot = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate and accelerate videos from folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'Dados/flood-video-collection-stamped' # `time-stamped` local collection source folder\n",
    "to_base_folder = 'Dados/flood-video-collection-date' # concatenated local collection destination folder\n",
    "overwrite_concat = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General purpose parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = '.mp4'\n",
    "report_freq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0.1 · Count blobs and download bytes · ***Preparation Step***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Blobs Searched: 3250\n",
      "\n",
      "  · Blobs (Matched): 3250\n",
      "\n",
      "  · Giga Bytes (Matched): 3.264 GB\n",
      "\n",
      "* TIME TO EXECUTE: 3.069 s\n"
     ]
    }
   ],
   "source": [
    "blobs = gcs_list_folder(prefix, ext, bucket_name, print_each=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 · Download  blobs in Cloud Storage bucket to folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download blobs in `bucket_name` matching `prefix` to local `folder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREFIX: comando/ · RUNNING: 36.0 min · RATE: 0.6649 s / file · FINISH-ESTIMATE: 0.0 min · PROGRESS: 3250/3250 · DOWNLOADS: 3246/3250\n",
      "\n",
      "* TIME TO EXECUTE: 2163.6748 s\n"
     ]
    }
   ],
   "source": [
    "timer = Timer()\n",
    "\n",
    "gcs.download_to_folder(\n",
    "    folder, prefix, delimiter, bucket_name,\n",
    "    overwrite_download, report_freq\n",
    ")\n",
    "\n",
    "timer.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 · Annotate videos with dinamic timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add clock timestamp to nested video files in `folder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIDEO TIMESTAMP ANNOTATION · DONE: 13405/13407 · SUCCESS: 3246/13407\n",
      "ANNOTATE VIDEO TIMESTAMP FAILED. FILE ALREADY EXISTS · FILE: Dados/flood-video-collection-stamped/polygons/manual/8/267/CODE267 2023-02-08 15-55-00.mp4\n",
      "ANNOTATE VIDEO TIMESTAMP FAILED. FILE ALREADY EXISTS · FILE: Dados/flood-video-collection-stamped/polygons/manual/8/267/CODE267 2023-02-08 16-20-00.mp4\n",
      "\n",
      "* TIME TO EXECUTE: 1638.281 s\n"
     ]
    }
   ],
   "source": [
    "timer = Timer()\n",
    "\n",
    "writer.annot_folder_nested(folder, to_folder, ext, overwrite_annot, report_freq)\n",
    "\n",
    "timer.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 · Concatenate and accelerate videos from folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate videos by date from nested folders in `base_folder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONCAT VIDEOS BY DATE FROM NESTED FOLDERS · DONE: 350/352 · FOLDER: polygons/manual/75/2140\n",
      "CONCAT VIDEOS BY DATE FROM FOLDER (FAILED) · FILE ALREADY EXISTS · FILE: CODE120 2023-02-08.mp4\n",
      "CONCAT VIDEOS BY DATE FROM FOLDER (FAILED) · FILE ALREADY EXISTS · FILE: CODE267 2023-02-08.mp4\n",
      "\n",
      "* TIME TO EXECUTE: 1452.5147 s\n"
     ]
    }
   ],
   "source": [
    "timer = Timer()\n",
    "\n",
    "writer_speed.concatenate_videos_from_nested_folders_by_date(\n",
    "    base_folder, to_base_folder, ext, overwrite_concat, report_freq\n",
    ")\n",
    "\n",
    "timer.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
