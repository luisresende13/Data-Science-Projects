{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Storage buckets image and video processing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luisr\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from IPython.display import clear_output as co\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Cloud Storage wrapper module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.googlecloudstorage import GCS, storage\n",
    "\n",
    "sa_json = '../../../../Apps/Python/cams-rio/auth/octacity-iduff.json' # 'auth/pluvia-sa.json'\n",
    "user_project = 'octacity'\n",
    "default_bucket_name = 'city-camera-images'\n",
    "\n",
    "gcs = GCS(sa_json, user_project, default_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image classification settings - Histogram max. percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.histogram import HistogramClassifier\n",
    "\n",
    "threshold=0.6\n",
    "clf_hist = HistogramClassifier(threshold)\n",
    "\n",
    "skipper = clf_hist.is_histogram_clustered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python module: Controlled pipeline execution (pandas dataframe based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.controlled_pipeline import ControlledPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXAMPLE USAGE · Google Cloud Storage image processing python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.gcs_image import Video\n",
    "\n",
    "# skipper = None\n",
    "\n",
    "video = Video(shape=(854, 480), fps=3, ext='.mp4', codec='mp4v', skipper=skipper, gcs=gcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload video from folder of images in cloud storage bucket  (simple example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* VIDEO UPLOAD SUCCESS · IMAGES: 7 · BLOB: test/TEST.mp4 · BUCKET: city-camera-images\n",
      "EXECUTION TIME: 1.7714 s\n"
     ]
    }
   ],
   "source": [
    "folder, from_bucket_name = 'flood/1635/2023-02-10 18:10:00/', np.nan #'city-camera-images'\n",
    "to_blob_name, to_bucket_name = 'test/TEST.mp4', 'city-camera-images' # 'flood-video-collection'\n",
    "\n",
    "s = time()\n",
    "video.upload_from_bucket_folder_of_images(\n",
    "    folder, from_bucket_name, to_blob_name, to_bucket_name,\n",
    "    ext='.jpg', content_type='video/mp4',\n",
    "    overwrite=True, report=True\n",
    "); t = time() - s\n",
    "\n",
    "print(f'EXECUTION TIME: {round(t, 4)} s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Storage buckets image and video processing pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load adapted cameras dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cameras = pd.read_csv('../../../../Apps/Python/cams-rio/static/city/cameras.csv')\n",
    "\n",
    "# cameras = pd.read_csv('static/city/cameras.csv')\n",
    "cameras['Codigo'] = cameras['Codigo'].astype(int)\n",
    "cameras['cluster_id'] = cameras['cluster_id'].astype(int)\n",
    "cameras.set_index('Codigo', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Folder to blob name settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_to_blob_name(folder, folder_map={}, polygon_blobs=[]):\n",
    "    if folder.endswith('/'): folder = folder[:-1]\n",
    "    info = folder.split('/')\n",
    "    prefix, code, stamp = '/'.join(info[:-2]), info[-2], info[-1]\n",
    "    to_prefix = folder_map[prefix]\n",
    "    if prefix in polygon_blobs:\n",
    "        polygon_id = cameras.loc[int(code), 'cluster_id']\n",
    "        return f'{to_prefix}/{polygon_id}/{code}/CODE{code} {stamp}.mp4'\n",
    "    return f'{to_prefix}/{code}/CODE{code} {stamp}.mp4'\n",
    "\n",
    "def blob_name_to_blob_name(blob_name, folder_map={}, polygon_blobs=[]):\n",
    "    info = blob_name.split('/')\n",
    "    prefix, code, filename = '/'.join(info[:-2]), info[-2], info[-1]\n",
    "    if prefix not in folder_map: return blob_name\n",
    "    to_prefix = folder_map[prefix]\n",
    "    if prefix in polygon_blobs:\n",
    "        polygon_id = cameras.loc[int(code), 'cluster_id']\n",
    "        return f'{to_prefix}/{polygon_id}/{code}/{filename}'\n",
    "    return f'{to_prefix}/{code}/{filename}'\n",
    "\n",
    "folder_map = {\n",
    "    'pics': 'polygons/flood-unlabeled',\n",
    "    'manual/2023-08-02': 'polygons/manual',\n",
    "    'flood': 'polygons/flood',\n",
    "    'rain': 'rain',\n",
    "    'auto/flood': 'polygons/flood',\n",
    "    'auto/rain': 'rain',\n",
    "}\n",
    "\n",
    "polygon_blobs = ['manual/2023-08-02', 'flood', 'pics', 'auto/flood']\n",
    "\n",
    "folder_to_blob_name_map = lambda folder: folder_to_blob_name(folder, folder_map, polygon_blobs)\n",
    "blob_name_map = lambda blob_name: blob_name_to_blob_name(blob_name, folder_map, polygon_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Move videos between Cloud Storage buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List blobs in bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Blobs Searched: 514831\n",
      "\n",
      " · Results: 7679\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'city-camera-images'\n",
    "prefix = ''\n",
    "delimiter = None\n",
    "ext = '.mp4'\n",
    "\n",
    "print_each = 1000\n",
    "\n",
    "blobs = gcs.list_blobs(prefix, delimiter, bucket_name)\n",
    "\n",
    "names = []\n",
    "for i, blob in enumerate(blobs):\n",
    "    if blob.name.endswith(ext): names.append(blob.name)\n",
    "    if (i + 1) % print_each == 0: print(f'\\n- Blobs Searched: {i + 1}'); co(True)\n",
    "\n",
    "print(f'\\n- Blobs Searched: {i + 1}')\n",
    "print(f'\\n · Results: {len(names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build video files control dataset from blobs names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = pd.DataFrame(names, columns=['blob_name'])\n",
    "\n",
    "control['from_bucket_name'] = None\n",
    "control['to_blob_name'] = control['blob_name'].map(blob_name_map)\n",
    "control['to_bucket_name'] = 'flood-video-collection'\n",
    "control['content_type'] = 'video/mp4'\n",
    "control['overwrite'] = False\n",
    "control['status'] = 'PENDING'\n",
    "\n",
    "control_path = f'Dados/blobs/video_control.csv'\n",
    "\n",
    "control.to_csv(control_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run move-videos pipeline using control dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- PROGRESS: 7450 / 7452 ops · PROGRESS-PRCT: 100.0 %\n",
      "\n",
      "- TOTAL: 15321 / 15323 ops · TOTAL-PRCT: 100.0 %\n",
      "\n",
      "- RUNNING: 353.1 min · EXPECT-FINISH: 0.1 min · RATE: 0.3517 ops / s\n",
      "\n",
      "* BLOBS CONTROL UPDATE SUCCESSFUL!\n",
      "\n",
      "* VIDEO UPLOAD SUCCESS · IMAGES: 6 · BLOB: rain/99/CODE99 2023-02-10 23:45:01.mp4 · BUCKET: flood-video-collection\n",
      "\n",
      "* VIDEO UPLOAD SUCCESS · IMAGES: 5 · BLOB: rain/99/CODE99 2023-02-11 00:30:01.mp4 · BUCKET: flood-video-collection\n",
      "\n",
      "- PROGRESS: 7451 / 7452 ops · PROGRESS-PRCT: 100.0 %\n",
      "\n",
      "- TOTAL: 15322 / 15323 ops · TOTAL-PRCT: 100.0 %\n",
      "\n",
      "- RUNNING: 353.1 min · EXPECT-FINISH: 0.0 min · RATE: 0.3517 ops / s\n",
      "\n",
      "* BLOBS CONTROL UPDATE SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "save_each = 50\n",
    "report_each = 10\n",
    "\n",
    "control_path = f'Dados/blobs/video_control.csv'\n",
    "# control_func = upload_from_blob_name\n",
    "\n",
    "params_fields = ['blob_name', 'from_bucket_name', 'to_blob_name', 'to_bucket_name', 'overwrite']\n",
    "query = {'status': ['PENDING', 'UPLOAD_FAILED']}\n",
    "status_field = 'status'\n",
    "status_options = ['UPLOAD_FAILED', 'UPLOADED']\n",
    "error_flag = 'ERROR'\n",
    "\n",
    "pipe = ControlledPipeline(\n",
    "    control_path, control_func, params_fields,\n",
    "    query, status_field, status_options, error_flag\n",
    ")\n",
    "\n",
    "pipe.run(report_each, save_each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload blobs control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UPLOADED         15322\n",
       "UPLOAD_FAILED        1\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "control_path = f'Dados/blobs/video_control.csv'\n",
    "\n",
    "control = pd.read_csv(control_path)\n",
    "\n",
    "control['status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Write video from folder of images in Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List blobs in bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'city-camera-images'\n",
    "prefix = ''\n",
    "delimiter = None\n",
    "ext = '/'\n",
    "\n",
    "drop_first = False\n",
    "print_each = 1000\n",
    "\n",
    "blobs = gcs.list_blobs(prefix, delimiter, bucket_name)\n",
    "\n",
    "names = []\n",
    "for i, blob in enumerate(blobs):\n",
    "    if blob.name.endswith(ext): names.append(blob.name)\n",
    "    if (i + 1) % print_each == 0: print(f'\\n- Blobs Searched: {i + 1}'); co(True)\n",
    "\n",
    "print(f'\\n- Blobs Searched: {i + 1}')\n",
    "if drop_first: print(f'\\n · First item excluded: {names[0]}'); del names[0]\n",
    "print(f'\\n · Results: {len(names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build videos control dataset from blobs names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = pd.DataFrame(names, columns=['folder'])\n",
    "\n",
    "control = control[control['folder'] != 'pics/']\n",
    "\n",
    "control['from_bucket_name'] = 'city-camera-images'\n",
    "control['to_blob_name'] = control['folder'].map(folder_to_blob_name_map)\n",
    "control['to_bucket_name'] = 'flood-video-collection'\n",
    "control['ext'] = '.jpg'\n",
    "control['content_type'] = 'video/mp4'\n",
    "control['overwrite'] = True\n",
    "control['status'] = 'PENDING'\n",
    "\n",
    "control_path = f'Dados/blobs/folders_to_video_control.csv'\n",
    "\n",
    "control.to_csv(control_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run image-video conversion pipeline using control dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- PROGRESS: 7450 / 7452 ops · PROGRESS-PRCT: 100.0 %\n",
      "\n",
      "- TOTAL: 15321 / 15323 ops · TOTAL-PRCT: 100.0 %\n",
      "\n",
      "- RUNNING: 353.1 min · EXPECT-FINISH: 0.1 min · RATE: 0.3517 ops / s\n",
      "\n",
      "* BLOBS CONTROL UPDATE SUCCESSFUL!\n",
      "\n",
      "* VIDEO UPLOAD SUCCESS · IMAGES: 6 · BLOB: rain/99/CODE99 2023-02-10 23:45:01.mp4 · BUCKET: flood-video-collection\n",
      "\n",
      "* VIDEO UPLOAD SUCCESS · IMAGES: 5 · BLOB: rain/99/CODE99 2023-02-11 00:30:01.mp4 · BUCKET: flood-video-collection\n",
      "\n",
      "- PROGRESS: 7451 / 7452 ops · PROGRESS-PRCT: 100.0 %\n",
      "\n",
      "- TOTAL: 15322 / 15323 ops · TOTAL-PRCT: 100.0 %\n",
      "\n",
      "- RUNNING: 353.1 min · EXPECT-FINISH: 0.0 min · RATE: 0.3517 ops / s\n",
      "\n",
      "* BLOBS CONTROL UPDATE SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "save_each = 50\n",
    "report_each = 10\n",
    "\n",
    "control_path = f'Dados/blobs/folders_to_video_control.csv'\n",
    "control_func = video.upload_from_bucket_folder_of_images\n",
    "\n",
    "params_fields = ['folder', 'from_bucket_name', 'to_blob_name', 'to_bucket_name', 'ext', 'content_type', 'overwrite']\n",
    "query = {'status': ['PENDING', 'UPLOAD_FAILED']}\n",
    "status_field = 'status'\n",
    "status_options = ['UPLOAD_FAILED', 'UPLOADED']\n",
    "error_flag = 'ERROR'\n",
    "\n",
    "pipe = ControlledPipeline(\n",
    "    control_path, control_func, params_fields,\n",
    "    query, status_field, status_options, error_flag\n",
    ")\n",
    "\n",
    "pipe.run(report_each, save_each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload blobs control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UPLOADED         15322\n",
       "UPLOAD_FAILED        1\n",
       "Name: status, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "control_path = 'Dados/blobs/folders_to_video_control.csv'\n",
    "\n",
    "control = pd.read_csv(control_path)\n",
    "\n",
    "control['status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Check folder/file correspondence between buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List folders in origin bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Blobs Searched: 514831\n",
      "\n",
      " · Results: 15323\n",
      "\n",
      "- BLOB: pics/ (excluded)\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'city-camera-images'\n",
    "prefix = '' # search all blobs\n",
    "delimiter = None\n",
    "ext = '/'\n",
    "\n",
    "drop_first = False\n",
    "print_each = 1000\n",
    "\n",
    "blobs = gcs.list_blobs(prefix, delimiter, bucket_name)\n",
    "\n",
    "names = []\n",
    "for i, blob in enumerate(blobs):\n",
    "    if blob.name.endswith(ext): names.append(blob.name)\n",
    "    if (i + 1) % print_each == 0: print(f'\\n- Blobs Searched: {i + 1}'); co(True)\n",
    "\n",
    "print(f'\\n- Blobs Searched: {i + 1}')\n",
    "if drop_first: print(f'\\n · First item excluded: {names[0]}'); del names[0]\n",
    "print(f'\\n · Results: {len(names)}')\n",
    "\n",
    "drop_if_exists = ['pics/', 'flood/', 'rain/', 'comando/', 'waze/']\n",
    "\n",
    "if len(drop_if_exists):\n",
    "    for name in drop_if_exists:\n",
    "        if name in names:\n",
    "            print(f'\\n- BLOB: {name} (excluded)')\n",
    "            del names[names.index(name)]\n",
    "        \n",
    "origin_folders = names[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert folder to correspondent blob name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_blob_names_corr = pd.Series(origin_folders).map(folder_to_blob_name_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List blobs in origin bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Blobs Searched: 514831\n",
      "\n",
      " · Results: 7679\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'city-camera-images'\n",
    "prefix = '' # search all blobs\n",
    "delimiter = None\n",
    "ext = '.mp4'\n",
    "\n",
    "drop_first = False\n",
    "print_each = 1000\n",
    "\n",
    "blobs = gcs.list_blobs(prefix, delimiter, bucket_name)\n",
    "\n",
    "names = []\n",
    "for i, blob in enumerate(blobs):\n",
    "    if blob.name.endswith(ext): names.append(blob.name)\n",
    "    if (i + 1) % print_each == 0: print(f'\\n- Blobs Searched: {i + 1}'); co(True)\n",
    "\n",
    "print(f'\\n- Blobs Searched: {i + 1}')\n",
    "if drop_first: print(f'\\n · First item excluded: {names[0]}'); del names[0]\n",
    "print(f'\\n · Results: {len(names)}')\n",
    "\n",
    "drop_if_exists = ['pics/', 'flood/', 'rain/', 'comando/', 'waze/']\n",
    "\n",
    "if len(drop_if_exists):\n",
    "    for name in drop_if_exists:\n",
    "        if name in names:\n",
    "            print(f'\\n- BLOB: {name} (excluded)')\n",
    "            del names[names.index(name)]\n",
    "        \n",
    "origin_blob_names = names[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert blob name to correspondent blob name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_blob_names_corr = pd.Series(origin_blob_names).map(blob_name_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List blobs in destination bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Blobs Searched: 32651\n",
      "\n",
      " · Results: 32651\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'flood-video-collection'\n",
    "prefix = ''\n",
    "delimiter = None\n",
    "ext = '.mp4'\n",
    "\n",
    "drop_first = False\n",
    "print_each = 100\n",
    "\n",
    "blobs = gcs.list_blobs(prefix, delimiter, bucket_name)\n",
    "\n",
    "names = []\n",
    "for i, blob in enumerate(blobs):\n",
    "    if (i+1) % print_each == 0: print(f'\\n- Blobs Searched: {i+1}'); co(True)\n",
    "    if blob.name.endswith(ext): names.append(blob.name)\n",
    "\n",
    "\n",
    "print(f'\\n- Blobs Searched: {i+1}')\n",
    "if drop_first:\n",
    "    print(f'\\n · First item excluded: {names[0]}'); del names[0]\n",
    "print(f'\\n · Results: {len(names)}')\n",
    "    \n",
    "destination_blob_names = pd.Series(names[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Difference from destination (folder-mp4): 0\n",
      "- Difference from source (folder-mp4): 17329\n",
      "- Difference from destination (mp4): 111\n",
      "- Difference from source (mp4): 25083\n"
     ]
    }
   ],
   "source": [
    "origin_set = set(destination_blob_names_corr)\n",
    "origin_blobs_set = set(origin_blob_names_corr)\n",
    "destination_set = set(destination_blob_names)\n",
    "\n",
    "dest_diff = list(origin_set.difference(destination_set))\n",
    "diff = list(destination_set.difference(origin_set))\n",
    "orig_diff = list(origin_blobs_set.difference(destination_set))\n",
    "dest_orig_diff = list(destination_set.difference(origin_blobs_set))\n",
    "\n",
    "print(f'- Difference from destination (folder-mp4): {len(dest_diff)}')\n",
    "print(f'- Difference from source (folder-mp4): {len(diff)}')\n",
    "print(f'- Difference from destination (mp4): {len(orig_diff)}')\n",
    "print(f'- Difference from source (mp4): {len(dest_orig_diff)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check missing blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Blob Name Level: 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "comando    98\n",
       "waze       13\n",
       "Name: 0, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Blob Name Level: 1\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "buraco_na_pista                67\n",
       "vazamento                      14\n",
       "acidente_enguiço_sem_vítima     8\n",
       "accident_major                  7\n",
       "accident_minor                  6\n",
       "operação_policial               6\n",
       "incêndio_em_veículo             3\n",
       "Name: 1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Blob Name Level: 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91700                                   20\n",
       "92182                                   19\n",
       "92140                                    9\n",
       "92146                                    8\n",
       "90535                                    7\n",
       "90516                                    7\n",
       "92227                                    6\n",
       "d16d470e-5cc5-4489-9af9-0c47b3178c5b     6\n",
       "93123                                    6\n",
       "92395                                    6\n",
       "91305                                    5\n",
       "93121                                    3\n",
       "92387                                    2\n",
       "00fcf469-98c9-44b1-88b1-e3d2432d56bd     2\n",
       "1ea4ec2d-ecfc-42f3-878a-1b085eeed3b1     1\n",
       "474f3057-9ea3-42b4-8510-f0fe8a601368     1\n",
       "74c4dc90-1857-48b0-87c1-faba429bf162     1\n",
       "1ef454c6-05ba-42f7-b3aa-6d7394a4c916     1\n",
       "b7770a0d-d063-4228-9107-f130649a6481     1\n",
       "Name: 2, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left = pd.DataFrame([name.split('/') for name in orig_diff])\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'\\nBlob Name Level: {i}\\n')\n",
    "    display(left[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXTRA: Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def average_time(func, n=100):\n",
    "    s = time()\n",
    "    for i in range(n): func()\n",
    "    t = time()\n",
    "    dt = t - s\n",
    "    avg = dt / n\n",
    "    return avg\n",
    "\n",
    "query = {\n",
    "    'folder': 'manual/2023-08-02/1119/2023-02-08 15:55:00/',\n",
    "    'to_blob_name': 'test/TEST.mp4',\n",
    "    'from_bucket_name': 'city-camera-images',\n",
    "    'to_bucket_name': 'city-camera-images', # 'flood-video-collection'\n",
    "    'ext': '.jpg',\n",
    "    'content_type': 'video/mp4',\n",
    "    'overwrite': True,\n",
    "    'report': False,\n",
    "}\n",
    "\n",
    "def local_upload_func():\n",
    "    video.upload_from_bucket_folder_of_images(**query)\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "baseurl = 'http://127.0.0.1:5001'\n",
    "url = f'{baseurl}/write-video' # ?{urlencode(query)}\n",
    "\n",
    "def api_upload_func(_id=None):\n",
    "    res = requests.get(url, params=query)\n",
    "    print(_id, res.status_code, res.reason, res.json()['uploaded'])\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "def run_many(func, params, workers=None):\n",
    "    if workers is None: workers = len(params)\n",
    "    pool = multiprocessing.Pool(processes=workers)\n",
    "    s = time()\n",
    "    pool.starmap(func, params); t = time()\n",
    "    return (t - s) / len(params)\n",
    "\n",
    "n = 2\n",
    "params = [(i,) for i in range(n)]\n",
    "\n",
    "avg_par = run_many(api_upload_func, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
